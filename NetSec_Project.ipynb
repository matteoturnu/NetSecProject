{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoturnu/NetSecProject/blob/main/NetSec_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Studying features correlation and distribution"
      ],
      "metadata": {
        "id": "qQWe616NKn6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepararing the dataset"
      ],
      "metadata": {
        "id": "Iu5pqOShFoUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the needed libraries"
      ],
      "metadata": {
        "id": "uVGww4jlq121"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score"
      ],
      "metadata": {
        "id": "XN43paX9p6bg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and setting the dataset"
      ],
      "metadata": {
        "id": "kPoUXal5BeCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/matteoturnu/NetSecProject/refs/heads/main/BenignAndMaliciousDataset.csv'\n",
        "traffic_df = pd.read_csv(url)\n",
        "print(traffic_df.shape)\n",
        "\n",
        "# We only select numeric and boolean features\n",
        "numeric_dataset = traffic_df.select_dtypes(include=['number','boolean'])\n",
        "\n",
        "# We remove Domain because it is an incremental ID\n",
        "new_numeric_dataset = numeric_dataset.drop(columns='Domain')\n",
        "# we remove Ip and ASN because they are based on strings \"de facto\" so it doesn't make sense to study correlation\n",
        "excluded_features = ['Ip', 'ASN']\n",
        "new_numeric_dataset = new_numeric_dataset.drop(columns=excluded_features)"
      ],
      "metadata": {
        "id": "r2oALLJYKsck",
        "outputId": "3ec9bef3-7a70-4aa9-9aeb-72e5c0c2db23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90000, 34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating features weight: ANOVA test\n",
        "UPDATE: ANOVA test can't be used since samples have no Gaussian distribution!\n",
        "\n",
        "Maybe move to this section gaussian test on samples distribution\n"
      ],
      "metadata": {
        "id": "HS4hhN1uBhKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's suppose \"target\" columns contains 0 for benign and 1 for malicious traffic\n",
        "# NOTE: actually the dataset uses \"0\" for malicious classes and \"1\" for benign ones\n",
        "target = 'Class'\n",
        "feature_columns = [col for col in new_numeric_dataset.columns if col != target]\n",
        "\n",
        "# ANOVA test for each numeric feature\n",
        "p_values = {}\n",
        "for feature in feature_columns:\n",
        "    # split data into bening and malicious groups\n",
        "    group_benign = new_numeric_dataset[new_numeric_dataset[target] == 0][feature]\n",
        "    group_malicious = new_numeric_dataset[new_numeric_dataset[target] == 1][feature]\n",
        "    # applying test\n",
        "    stat, p_value = stats.f_oneway(group_benign, group_malicious)\n",
        "    # save p-value for each feature\n",
        "    p_values[feature] = p_value\n",
        "\n",
        "    # evaluate p-value\n",
        "    if p_value < 0.01:  # significance level (alpha)\n",
        "        print(f'Feature \"{feature}\" is significant (p-value: {p_value:.3e})')\n",
        "    else:\n",
        "        print(f'La feature {feature} is NOT significant (p-value: {p_value:.3e})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vbsQUJiXzHL",
        "outputId": "8e74c5ea-c20d-4bff-978d-9c06518ce8a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature \"MXDnsResponse\" is significant (p-value: 0.000e+00)\n",
            "Feature \"TXTDnsResponse\" is significant (p-value: 0.000e+00)\n",
            "Feature \"HasSPFInfo\" is significant (p-value: 0.000e+00)\n",
            "Feature \"HasDkimInfo\" is significant (p-value: 6.456e-04)\n",
            "Feature \"HasDmarcInfo\" is significant (p-value: 1.391e-62)\n",
            "Feature \"DomainInAlexaDB\" is significant (p-value: 3.066e-94)\n",
            "Feature \"CommonPorts\" is significant (p-value: 0.000e+00)\n",
            "Feature \"CreationDate\" is significant (p-value: 0.000e+00)\n",
            "Feature \"LastUpdateDate\" is significant (p-value: 0.000e+00)\n",
            "Feature \"HttpResponseCode\" is significant (p-value: 0.000e+00)\n",
            "La feature SubdomainNumber is NOT significant (p-value: 1.102e-02)\n",
            "Feature \"Entropy\" is significant (p-value: 0.000e+00)\n",
            "Feature \"EntropyOfSubDomains\" is significant (p-value: 2.358e-17)\n",
            "Feature \"StrangeCharacters\" is significant (p-value: 0.000e+00)\n",
            "Feature \"IpReputation\" is significant (p-value: 3.377e-219)\n",
            "La feature DomainReputation is NOT significant (p-value: 4.411e-01)\n",
            "Feature \"ConsoantRatio\" is significant (p-value: 0.000e+00)\n",
            "Feature \"NumericRatio\" is significant (p-value: 0.000e+00)\n",
            "Feature \"SpecialCharRatio\" is significant (p-value: 7.998e-312)\n",
            "Feature \"VowelRatio\" is significant (p-value: 0.000e+00)\n",
            "Feature \"ConsoantSequence\" is significant (p-value: 9.977e-90)\n",
            "Feature \"VowelSequence\" is significant (p-value: 0.000e+00)\n",
            "Feature \"NumericSequence\" is significant (p-value: 0.000e+00)\n",
            "Feature \"SpecialCharSequence\" is significant (p-value: 3.249e-76)\n",
            "Feature \"DomainLength\" is significant (p-value: 0.000e+00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous test shows features \"DomainReputation\" and \"SubdomainNumber\" are not important for determining the class"
      ],
      "metadata": {
        "id": "vb2HybcXBQGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_numeric_dataset = new_numeric_dataset.drop(columns=['DomainReputation', 'SubdomainNumber'])"
      ],
      "metadata": {
        "id": "rBCcWUmvBNau"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bening and malicious traffic distribution based on remaining features"
      ],
      "metadata": {
        "id": "krEey93WBnI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i, feature in enumerate(new_numeric_dataset, 1):\n",
        "    plt.subplot(7, 5, i)\n",
        "    plot = sns.histplot(data=traffic_df, x=feature, hue='Class', multiple='dodge', palette='Set1', bins=25, stat='percent')\n",
        "\n",
        "    if traffic_df[feature].nunique() == 2:  # Check if the current feature is boolean\n",
        "        plt.xticks([0, 1])\n",
        "        plt.xlim(-0.5, 1.5)\n",
        "\n",
        "    plt.title(f'{feature}', fontweight='bold')\n",
        "    plt.ylabel('Total [%]')\n",
        "\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.minorticks_on()\n",
        "    plt.grid(True, which='minor', linestyle=':', linewidth=0.5)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9HQ_ppVlSqBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphs show some features are equally distributed among malicious and bening samples"
      ],
      "metadata": {
        "id": "zUuDg997CNR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove overlapping features\n",
        "new_numeric_dataset = new_numeric_dataset.drop(columns=['HasDkimInfo', 'HasDmarcInfo', 'DomainInAlexaDB', 'CommonPorts', 'EntropyOfSubDomains', 'IpReputation'])"
      ],
      "metadata": {
        "id": "AUVBH8L9CN71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Studying feature-target correlation"
      ],
      "metadata": {
        "id": "UONRF60UDyr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = new_numeric_dataset.corr()\n",
        "class_corr = corr_matrix[['Class']]\n",
        "plt.figure(figsize=(1, 6))\n",
        "sns.heatmap(class_corr, annot=True, linewidths=0.5, fmt='.2f')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KqaP6jDdyib7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chosen features (based on high correlation with the class)\n",
        "- TXTDnsResponse\n",
        "- HasSPFInfo\n",
        "- StrangeCharacters\n",
        "- ConsonantRatio\n",
        "- NumericRatio\n",
        "- VowelRatio\n",
        "- NumericSequence"
      ],
      "metadata": {
        "id": "sYTzbIZWDo1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Studying pairwise feature correlation"
      ],
      "metadata": {
        "id": "IgOSnI1JEKP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['TXTDnsResponse', 'HasSPFInfo', 'StrangeCharacters', 'ConsoantRatio', 'NumericRatio', 'VowelRatio', 'NumericSequence']\n",
        "new_numeric_dataset = new_numeric_dataset[features]\n",
        "\n",
        "corr_matrix = new_numeric_dataset.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, linewidth=0.5, fmt='.2f')"
      ],
      "metadata": {
        "id": "WU7o2y-FdvMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, looking at correlation between features themselves:\n",
        "- choosing consonantRatio against NumericRatio\n",
        "- keeping TXTDnsResponse against HasSPFInfo\n",
        "- choosing NumericSequence against consonantRatio (4 features left)\n",
        "\n",
        "Left: NumericSequence, TXTDnsResponse, StrangeCharacters, VowelRatio (next test: maybe remove NumericSequence or VowelRatio)"
      ],
      "metadata": {
        "id": "bUPEzk5RgblG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_features = ['TXTDnsResponse', 'StrangeCharacters', 'VowelRatio', 'NumericSequence']"
      ],
      "metadata": {
        "id": "A8qrDYnU4Vtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing the ML classifier"
      ],
      "metadata": {
        "id": "Ax4BkV2l4v13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to evaluate how much the samples distribution is a Gaussian one\n"
      ],
      "metadata": {
        "id": "_stslIxPE6ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study_features=['TXTDnsResponse', 'StrangeCharacters', 'VowelRatio', 'NumericSequence', 'Class']\n",
        "dataset_study=traffic_df[study_features]\n",
        "dataset_ben= dataset_study[dataset_study['Class']==1]\n",
        "dataset_mal= dataset_study[dataset_study['Class']==0]\n",
        "\n",
        "# Esegui il test di Shapiro-Wilk\n",
        "stat, p_value = stats.shapiro(dataset_ben)\n",
        "\n",
        "print('Statistic:', stat)\n",
        "print('p-value:', p_value)\n",
        "\n",
        "if p_value > 0.05:\n",
        "    print('Distribuzione probabilmente normale (non rifiuto H0)')\n",
        "else:\n",
        "    print('Distribuzione non normale (rifiuto H0)')\n",
        "\n",
        "stat, p_value = stats.shapiro(dataset_mal)\n",
        "\n",
        "print('Statistic:', stat)\n",
        "print('p-value:', p_value)\n",
        "\n",
        "if p_value > 0.05:\n",
        "    print('Distribuzione probabilmente normale (non rifiuto H0)')\n",
        "else:\n",
        "    print('Distribuzione non normale (rifiuto H0)')"
      ],
      "metadata": {
        "id": "uj-xLtaMpXNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "i = 1\n",
        "for feature in final_features[1:]:\n",
        "  for j in range(2):\n",
        "    plt.subplot(3, 2, i)\n",
        "    if i % 2 == 1:\n",
        "      plot = sns.histplot(data=dataset_ben, x=feature, multiple='dodge', bins=25, stat='percent', color='blue')\n",
        "    else:\n",
        "      plot = sns.histplot(data=dataset_mal, x=feature, multiple='dodge', bins=25, stat='percent', color='red')\n",
        "    i += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "agCCaZUopxk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort of probability distribution for each feature. They are NO gaussian ones so it's not possible to suppose that data has Gaussian distribution"
      ],
      "metadata": {
        "id": "53H8ZMp66B4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-NN"
      ],
      "metadata": {
        "id": "u8kBJ0H4YpNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = traffic_df['Class']\n",
        "\n",
        "# k values to try\n",
        "k_values = [5, 10, 20, 50, 100, 200, 300]"
      ],
      "metadata": {
        "id": "ETdERQvdHPZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test for chosen features"
      ],
      "metadata": {
        "id": "tDgAAI-vOeGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-NN con tutte le 4 features"
      ],
      "metadata": {
        "id": "qGDieckVNI7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = traffic_df[final_features] # 4 features\n",
        "\n",
        "# X = traffic_df[final_features] # Sostituisci con le tue feature\n",
        "\n",
        "# Suddividi il dataset in 70% training e 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Normalizzazione delle feature per KNN\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Addestramento del modello\n",
        "k = 5\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Valutazione del modello\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Matrice di confusione\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "TP = cm[1, 1]  # Veri positivi\n",
        "TN = cm[0, 0]  # Veri negativi\n",
        "FP = cm[0, 1]  # Falsi positivi\n",
        "FN = cm[1, 0]  # Falsi negativi\n",
        "\n",
        "TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "P = TP/(TP+FP) if (TP+FP) > 0 else 0\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "print(f'True Positive Ratio: {TPR:.2f}')\n",
        "print(f'False Negative Ratio: {FNR:.2f}')\n",
        "print(f'Precision: {P:.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Test su accuratezza per diversi valori di k\n",
        "k_values = [1, 3, 5, 7, 9, 11]\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    print(f'k={k}, Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "\n",
        "# --- CURVA ROC E CALCOLO AUC ---\n",
        "# Prevedi le probabilità delle classi positive\n",
        "y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcola fpr, tpr e soglie per la ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Calcola AUC\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Traccia la curva ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mn_3TwJtaaRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-NN sulle 4 features prese singolarmente"
      ],
      "metadata": {
        "id": "0GTEeCM7NSaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for feature in X:\n",
        "    print(f\"\\nAnalizzando la feature: {feature}\")\n",
        "\n",
        "    feature_vector = traffic_df[feature].values.reshape(-1, 1)  # Reshape per ottenere una 2D array\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(feature_vector, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    k = 5\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Valutazione del modello\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    TP = cm[1, 1]  # Veri positivi\n",
        "    TN = cm[0, 0]  # Veri negativi\n",
        "    FP = cm[0, 1]  # Falsi positivi\n",
        "    FN = cm[1, 0]  # Falsi negativi\n",
        "\n",
        "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    P = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "    print(f'True Positive Ratio: {TPR:.2f}')\n",
        "    print(f'False Negative Ratio: {FNR:.2f}')\n",
        "    print(f'Precision: {P:.2f}')\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Prevedi le probabilità della classe positiva\n",
        "    y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calcola fpr, tpr e soglie per la ROC\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "    # Calcola AUC\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    # Traccia la curva ROC per ogni feature\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{feature} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Aggiungi dettagli al grafico\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  # Linea diagonale per classificatore casuale\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curves per le 4 feature')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dxBExOm3IVbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test for excluded features"
      ],
      "metadata": {
        "id": "oL80Fw2SGBbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = traffic_df[excluded_features]\n",
        "\n",
        "for feature in X:\n",
        "  print(f\"\\nAnalizzando la feature: {feature}\")\n",
        "\n",
        "  feature_vector = traffic_df[feature].values.reshape(-1, 1)  # Reshape per ottenere una 2D array\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(feature_vector, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  k = 5\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  # Model evaluation\n",
        "  y_pred = knn.predict(X_test)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  TP = cm[1, 1]  # veri positivi\n",
        "  TN = cm[0, 0]  # veri negativi\n",
        "  FP = cm[0, 1]  # falsi positivi\n",
        "  FN = cm[1, 0]  # falsi negativi\n",
        "\n",
        "  TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "  FNR = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "  P = TP/(TP+FP)\n",
        "\n",
        "  print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "  print(f'True Positive Ratio: {TPR:.2f}')\n",
        "  print(f'False Negative Ratio: {FNR:.2f}')\n",
        "  print(f'Precision: {P:.2f}')\n",
        "  print('Classification Report:')\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    print(f'k={k}, Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
      ],
      "metadata": {
        "id": "-xiCYK1HGFi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}